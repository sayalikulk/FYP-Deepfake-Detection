{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/d3/ecb4d108f6c1041d24842a345ee0123cd7f366ba75cf122601e856d42ba2/imutils-0.5.4.tar.gz\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/db/23/45/fc7424906880ffa9577a2a428b961f2b79e0e21d9f71e7e6bc\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 20.3.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5nSNJOMDURBR"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path='./Abhishek/faceforensics/original_sequences/youtube/c23/videos'\n",
    "path='./Abhishek/faceforensics/manipulated_sequences/NeuralTextures/c23/videos/'\n",
    "path_to_copy='./Sayali/FF_preprocessed_Sayali_Fake/'\n",
    "count=0\n",
    "for videos in os.listdir(path_to_copy):\n",
    "    a=0\n",
    "    p=os.path.join(path_to_copy,videos)\n",
    "    num_files = len([f for f in os.listdir(p)if os.path.isfile(os.path.join(p, f))])\n",
    "    \n",
    "    x=os.path.join(path,videos)\n",
    "    cap = cv2.VideoCapture(x)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if length!=0:\n",
    "        a=num_files/length\n",
    "    if a<.8:\n",
    "        count=count+1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ro9uvM2lQnw0"
   },
   "outputs": [],
   "source": [
    "def valid_x(x,a):\n",
    "    if x-a>=0:\n",
    "        return x-a\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def valid_y(y,a):\n",
    "    if y-a>=0:\n",
    "        return y-a\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def valid_w(x,w,a,width):\n",
    "    if x+w+2*a<=width:\n",
    "        return w+2*a\n",
    "    else:\n",
    "        return width-x\n",
    "\n",
    "def valid_h(y,h,a,height):\n",
    "    if y+h+2*a<=height:\n",
    "        return h+2*a\n",
    "    else:\n",
    "        return height-h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l9Qsc1kdUjTg"
   },
   "outputs": [],
   "source": [
    "def detect_faces(cascade, test_image,height,width, scaleFactor = 1.3):\n",
    "    # create a copy of the image to prevent any changes to the original one.\n",
    "    image_copy = test_image.copy()\n",
    "\n",
    "    #convert the test image to gray scale as opencv face detector expects gray images\n",
    "    gray_image = cv2.cvtColor(image_copy, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Applying the haar classifier to detect faces\n",
    "    faces_rect = cascade.detectMultiScale(gray_image, scaleFactor=scaleFactor, minNeighbors=5)\n",
    "    print(faces_rect)\n",
    "    n=0\n",
    "    l=[]\n",
    "    for (x, y, w, h) in faces_rect:\n",
    "        if w>n:\n",
    "            n=w\n",
    "            l=[x,y,w,h]\n",
    "    if(l==[]):\n",
    "      return\n",
    "    a=0.1*l[2]\n",
    "    x1=valid_x(l[0],a)\n",
    "    y1=valid_y(l[1],a)\n",
    "    w1=valid_w(x1,l[2],a,width)\n",
    "    h1=valid_h(y1,l[3],a,height)\n",
    "    return (int(x1),int(y1),int(w1),int(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZaPIdM6rZtQe"
   },
   "outputs": [],
   "source": [
    "path='./Abhishek/faceforensics/manipulated_sequences/NeuralTextures/c23/videos'\n",
    "path_to_copy='./Sayali/FF_preprocessed_Sayali_Fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "KXgsCAe4Ule9",
    "outputId": "a0d0c3bb-69e0-45f8-cdd7-f2b983ceb983",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631_551.mp4\n",
      "Directory not created.\n",
      "531_549.mp4\n",
      "Directory not created.\n",
      "852_834.mp4\n",
      "Directory not created.\n",
      "975_978.mp4\n",
      "./Sayali/FF_preprocessed_Sayali_Fake/975_978.mp4\n",
      "[[560 181 207 207]]\n",
      "(539, 160, 248, 248)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "OPENCV_OBJECT_TRACKERS = {\n",
    "    \"csrt\":cv2.TrackerCSRT_create,\n",
    "    \"kcf\":cv2.TrackerKCF_create,\n",
    "}\n",
    "\n",
    "haar_cascade_face=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "for videos in os.listdir(path):\n",
    "  tracker = OPENCV_OBJECT_TRACKERS[\"kcf\"]()\n",
    "\n",
    "  #for bounding box cordinates of obj we are going\n",
    "  initBB = None\n",
    "  print(videos)\n",
    "  p=os.path.join(path,videos)\n",
    "  to_copy=os.path.join(path_to_copy,videos)\n",
    "  try:\n",
    "    os.mkdir(to_copy)\n",
    "  except OSError as e:\n",
    "    if e.errno == errno.EEXIST:\n",
    "        print('Directory not created.')\n",
    "        continue\n",
    "    else:\n",
    "        raise\n",
    "  print(to_copy)\n",
    "  vs = cv2.VideoCapture(p)\n",
    "  #loop over frames from the video stream\n",
    "  flag = False\n",
    "  count=0\n",
    "  while True:\n",
    "      if count>1000:\n",
    "          break\n",
    "      ret,frame = vs.read()\n",
    "\n",
    "      if frame is None:\n",
    "          continue\n",
    "      if ret is False:\n",
    "          continue\n",
    "            \n",
    "      (H, W) = frame.shape[:2]\n",
    "\n",
    "\n",
    "      #we are curently tracking on object\n",
    "      if initBB is not None:\n",
    "          (success, box) = tracker.update(frame)\n",
    "          #if the tracking was a success\n",
    "          if ret and success:\n",
    "            (x, y, w, h) = [int(v) for v in box]\n",
    "            #cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            a=frame[y:y+h,x:x+w]\n",
    "            if(a.size>0):\n",
    "              cv2.imwrite(to_copy+'/'+str(count)+'.jpg',a)\n",
    "\n",
    "          #update the fps counter\n",
    "          fps.update()\n",
    "          fps.stop()\n",
    "\n",
    "          info = [\n",
    "              (\"Tracker\", \"kcf\"),\n",
    "              (\"Success\", \"Yes\" if success else \"No\"),\n",
    "              (\"FPS\", \"{:.2f}\".format(fps.fps())),\n",
    "          ]\n",
    "          \n",
    "          for (i, (k, v)) in enumerate(info):\n",
    "              text = \"{}: {}\".format(k, v)\n",
    "              cv2.putText(frame, text, (10, H - ((i * 20) + 20)),\n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "      #show the output frame\n",
    "      key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "      #we are selecting the bounding box so press \"s\" and click enter or space\n",
    "      #if you want again reselect then press escape\n",
    "      count=count+1\n",
    "      if flag==False:\n",
    "          initBB = detect_faces(haar_cascade_face, frame,H,W)\n",
    "          if(initBB==None):\n",
    "            continue\n",
    "          print(initBB)\n",
    "          flag = True\n",
    "          tracker.init(frame, initBB)\n",
    "          fps = FPS().start()\n",
    "\n",
    "      elif key == ord(\"q\"):\n",
    "          break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (66.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 66.7 MB 5.4 MB/s eta 0:00:013\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python) (1.17.3)\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25858 sha256=6cc3eef4e9a50cd9cb10b33088d77018413499d3650a3b2f27707d05bf1ac5fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/f5/0c/3a/61b992f7aa85de40f339e6d4970d91dddb103dd0ad6c5d58f2\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils, opencv-contrib-python\n",
      "Successfully installed imutils-0.5.4 opencv-contrib-python-4.5.5.64\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imutils opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import errno\n",
    "\n",
    "os.listdir()\n",
    "\n",
    "def valid_x(x,a):\n",
    "    if x-a>=0:\n",
    "        return x-a\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def valid_y(y,a):\n",
    "    if y-a>=0:\n",
    "        return y-a\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def valid_w(x,w,a,width):\n",
    "    if x+w+2*a<=width:\n",
    "        return w+2*a\n",
    "    else:\n",
    "        return width-x\n",
    "\n",
    "def valid_h(y,h,a,height):\n",
    "    if y+h+2*a<=height:\n",
    "        return h+2*a\n",
    "    else:\n",
    "        return height-h\n",
    "\n",
    "\n",
    "def detect_faces(cascade, test_image,height,width, scaleFactor = 1.3):\n",
    "    # create a copy of the image to prevent any changes to the original one.\n",
    "    image_copy = test_image.copy()\n",
    "\n",
    "    #convert the test image to gray scale as opencv face detector expects gray images\n",
    "    gray_image = cv2.cvtColor(image_copy, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Applying the haar classifier to detect faces\n",
    "    faces_rect = cascade.detectMultiScale(gray_image, scaleFactor=scaleFactor, minNeighbors=5)\n",
    "    print(faces_rect)\n",
    "    n=0\n",
    "    l=[]\n",
    "    for (x, y, w, h) in faces_rect:\n",
    "        if w>n:\n",
    "            n=w\n",
    "            l=[x,y,w,h]\n",
    "    if(l==[]):\n",
    "      return\n",
    "    a=0.1*l[2]\n",
    "    x1=valid_x(l[0],a)\n",
    "    y1=valid_y(l[1],a)\n",
    "    w1=valid_w(x1,l[2],a,width)\n",
    "    h1=valid_h(y1,l[3],a,height)\n",
    "    return (int(x1),int(y1),int(w1),int(h1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.6/dist-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.13.3; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python) (1.17.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deq3KKm3Uzzl",
    "outputId": "32f04585-5f34-4756-d41d-5bb2bebe1562"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerCSRT_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-268e6047a9a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m OPENCV_OBJECT_TRACKERS = {\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m\"csrt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackerCSRT_create\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"kcf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackerKCF_create\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"boosting\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackerBoosting_create\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerCSRT_create'"
     ]
    }
   ],
   "source": [
    "\n",
    "path='./Abhishek/faceforensics/original_sequences/youtube/c23/videos'\n",
    "path_to_copy='./Sayali/Time_analysis_real'\n",
    "\n",
    "OPENCV_OBJECT_TRACKERS = {\n",
    "    \"csrt\":cv2.TrackerCSRT_create,\n",
    "    \"kcf\":cv2.TrackerKCF_create,\n",
    "    \"boosting\": cv2.TrackerBoosting_create,\n",
    "    \"mil\": cv2.TrackerMIL_create,\n",
    "    \"tld\": cv2.TrackerTLD_create,\n",
    "    \"medianflow\": cv2.TrackerMedianFlow_create,\n",
    "    \"mosse\": cv2.TrackerMOSSE_create\n",
    "}\n",
    "\n",
    "haar_cascade_face=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['377.mp4',\n",
       " '172.mp4',\n",
       " '284.mp4',\n",
       " '180.mp4',\n",
       " '024.mp4',\n",
       " '168.mp4',\n",
       " '553.mp4',\n",
       " '890.mp4',\n",
       " '690.mp4',\n",
       " '295.mp4',\n",
       " '778.mp4',\n",
       " '893.mp4',\n",
       " '067.mp4',\n",
       " '246.mp4',\n",
       " '584.mp4',\n",
       " '002.mp4',\n",
       " '560.mp4',\n",
       " '994.mp4',\n",
       " '423.mp4',\n",
       " '460.mp4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import sample\n",
    "list1 = os.listdir(path)\n",
    "v=sample(list1,20)\n",
    "v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "J3anGBXbVolG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777.mp4\n",
      "[[238  90 177 177]]\n",
      "(220, 72, 212, 212)\n",
      "033.mp4\n",
      "[[368  55  76  76]\n",
      " [134 194  49  49]]\n",
      "(360, 47, 91, 91)\n",
      "314.mp4\n",
      "[[152 109 180 180]]\n",
      "(134, 91, 216, 216)\n",
      "737.mp4\n",
      "[[207  66 116 116]]\n",
      "(195, 54, 139, 139)\n",
      "255.mp4\n",
      "[[548 128 277 277]]\n",
      "(520, 100, 332, 332)\n",
      "539.mp4\n",
      "[[121  53 249 249]]\n",
      "(96, 28, 298, 298)\n",
      "545.mp4\n",
      "[[292 142 216 216]]\n",
      "(270, 120, 259, 259)\n",
      "321.mp4\n",
      "[[285 106  93  93]]\n",
      "(275, 96, 111, 111)\n",
      "791.mp4\n",
      "[[350  96 174 174]]\n",
      "(332, 78, 208, 208)\n",
      "422.mp4\n",
      "[[911 327 513 513]]\n",
      "(859, 275, 615, 615)\n",
      "349.mp4\n",
      "[[789  74 137 137]]\n",
      "(775, 60, 164, 164)\n",
      "260.mp4\n",
      "[[139 132 176 176]]\n",
      "(121, 114, 211, 211)\n",
      "574.mp4\n",
      "()\n",
      "[[212 121 134 134]]\n",
      "(198, 107, 160, 160)\n",
      "363.mp4\n",
      "[[313  96 116 116]]\n",
      "(301, 84, 139, 139)\n",
      "608.mp4\n",
      "[[240  57 148 148]]\n",
      "(225, 42, 177, 177)\n",
      "874.mp4\n",
      "[[804 300 397 397]]\n",
      "(764, 260, 476, 476)\n",
      "632.mp4\n",
      "[[722 172 346 346]]\n",
      "(687, 137, 415, 415)\n",
      "754.mp4\n",
      "[[449 122 224 224]]\n",
      "(426, 99, 268, 268)\n",
      "793.mp4\n",
      "[[460 106 274 274]]\n",
      "(432, 78, 328, 328)\n",
      "900.mp4\n",
      "[[270 135 128 128]]\n",
      "(257, 122, 153, 153)\n",
      "Total time required:\n",
      "631.1344051361084\n",
      "total frames:\n",
      "9333\n",
      "total faces detected:\n",
      "9292\n",
      "Success Rate:\n",
      "0.974310579846912\n",
      "Time per frame:\n",
      "0.06617745676167645\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t=0\n",
    "f=0\n",
    "\n",
    "fr_count=0\n",
    "\n",
    "for videos in v:\n",
    "    \n",
    "    tracker = OPENCV_OBJECT_TRACKERS[\"tld\"]()\n",
    "    \n",
    "    #for bounding box cordinates of obj we are going\n",
    "    initBB = None\n",
    "    print(videos)\n",
    "    p=os.path.join(path,videos)\n",
    "#     to_copy=os.path.join(path_to_copy,videos)\n",
    "\n",
    "#     try:\n",
    "#         os.mkdir(to_copy)\n",
    "#     except OSError as e:\n",
    "#         if e.errno == errno.EEXIST:\n",
    "#             print('Directory not created.')\n",
    "#             continue\n",
    "#         else:\n",
    "#             raise\n",
    "#     print(to_copy)\n",
    "    \n",
    "    vs = cv2.VideoCapture(p)\n",
    "    #loop over frames from the video stream\n",
    "    flag = False\n",
    "    count=0\n",
    "    e=0\n",
    "    while True:\n",
    "        \n",
    "        if count>1000:\n",
    "            break\n",
    "      \n",
    "        ret,frame = vs.read()\n",
    "        \n",
    "        if frame is None:\n",
    "            break\n",
    "        if ret is False:\n",
    "            break\n",
    "            \n",
    "        (H, W) = frame.shape[:2]\n",
    "        \n",
    "        #we are curently tracking on object\n",
    "        \n",
    "        if initBB is not None:\n",
    "            start=time.time()\n",
    "            (success, box) = tracker.update(frame)\n",
    "            end=time.time()\n",
    "            t=t+(end-start)\n",
    "            #if the tracking was a success\n",
    "            if ret and success:\n",
    "                (x, y, w, h) = [int(v) for v in box]\n",
    "                #cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                a=frame[y:y+h,x:x+w]\n",
    "                if(a.size>0):\n",
    "                    e=e+1\n",
    "\n",
    "            #update the fps counter\n",
    "            \n",
    "            fps.update()\n",
    "            fps.stop()\n",
    "\n",
    "#             info = [\n",
    "#                 (\"Tracker\", \"kcf\"),\n",
    "#                 (\"Success\", \"Yes\" if success else \"No\"),\n",
    "#                 (\"FPS\", \"{:.2f}\".format(fps.fps())),\n",
    "#             ]\n",
    "\n",
    "            #for (i, (k, v)) in enumerate(info):\n",
    "                #text = \"{}: {}\".format(k, v)\n",
    "                #cv2.putText(frame, text, (10, H - ((i * 20) + 20)),\n",
    "                    #cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        #we are selecting the bounding box so press \"s\" and click enter or space\n",
    "        #if you want again reselect then press escape\n",
    "        count=count+1\n",
    "        if flag==False:\n",
    "            initBB = detect_faces(haar_cascade_face, frame,H,W)\n",
    "            if(initBB==None):\n",
    "                continue\n",
    "            print(initBB)\n",
    "            flag = True\n",
    "            tracker.init(frame, initBB)\n",
    "            fps = FPS().start()\n",
    "    fr_count=fr_count+e\n",
    "    f=f+count\n",
    "    \n",
    "print(\"Total time required:\")\n",
    "print(t)\n",
    "print(\"total frames:\")\n",
    "print(f)\n",
    "print(\"total faces detected:\")\n",
    "print(fr_count)\n",
    "print(\"Success Rate:\")\n",
    "print(fr_count/9537)\n",
    "print(\"Time per frame:\")\n",
    "print(t/9537)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Haar Cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['777.mp4', '033.mp4', '314.mp4', '737.mp4', '255.mp4', '539.mp4', '545.mp4', '321.mp4', '791.mp4', '422.mp4', '349.mp4', '260.mp4', '574.mp4', '363.mp4', '608.mp4', '874.mp4', '632.mp4', '754.mp4', '793.mp4', '900.mp4']\n",
      "777.mp4\n",
      "033.mp4\n",
      "314.mp4\n",
      "737.mp4\n",
      "255.mp4\n",
      "539.mp4\n",
      "545.mp4\n",
      "321.mp4\n",
      "791.mp4\n",
      "422.mp4\n",
      "349.mp4\n",
      "260.mp4\n",
      "574.mp4\n",
      "363.mp4\n",
      "608.mp4\n",
      "874.mp4\n",
      "632.mp4\n",
      "754.mp4\n",
      "793.mp4\n",
      "900.mp4\n",
      "Total time required:\n",
      "142.09819078445435\n",
      "total frames:\n",
      "9537\n",
      "Total faces detected successfully:\n",
      "9537\n",
      "Success rate:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import imutils\n",
    "import numpy as np\n",
    "c='./Abhishek/faceforensics/original_sequences/youtube/c23/videos/722.mp4'\n",
    "# Load the Haar Cascade\n",
    "cascPath = 'haarcascade_frontalface_default.xml'\n",
    "\n",
    "# Create the Haar Cascade\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "v=['777.mp4',\n",
    " '033.mp4',\n",
    " '314.mp4',\n",
    " '737.mp4',\n",
    " '255.mp4',\n",
    " '539.mp4',\n",
    " '545.mp4',\n",
    " '321.mp4',\n",
    " '791.mp4',\n",
    " '422.mp4',\n",
    " '349.mp4',\n",
    " '260.mp4',\n",
    " '574.mp4',\n",
    " '363.mp4',\n",
    " '608.mp4',\n",
    " '874.mp4',\n",
    " '632.mp4',\n",
    " '754.mp4',\n",
    " '793.mp4',\n",
    " '900.mp4']\n",
    "\n",
    "print(v)\n",
    "t=0\n",
    "f=0\n",
    "fr_count=0;\n",
    "for videos in v:\n",
    "    print(videos)\n",
    "    p=os.path.join(path,videos)\n",
    "    # Read the Video\n",
    "    \n",
    "    video_capture = cv2.VideoCapture(p)\n",
    "    count=0\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        #count=count+1\n",
    "        ret, frame = video_capture.read()\n",
    "        if frame is None:\n",
    "            break\n",
    "        if ret is False:\n",
    "            break\n",
    "        (H, W) = frame.shape[:2]\n",
    "        if np.shape(frame) == ():\n",
    "            break\n",
    "        count=count+1\n",
    "        #print(count)\n",
    "        # Resize the Frame to improve speed\n",
    "        #frame = imutils.resize(frame, width=450)\n",
    "    \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        t1=time.time()\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.3,\n",
    "            minNeighbors=5,\n",
    "        )\n",
    "        t2=time.time()\n",
    "        t=t+(t2-t1)\n",
    "        n=0\n",
    "        for (x, y, w, h) in faces:\n",
    "            if w>n:\n",
    "                n=w\n",
    "                l=[x,y,w,h]\n",
    "                \n",
    "        a=0.1*l[2]\n",
    "        x=int(valid_x(l[0],a))\n",
    "        y=int(valid_y(l[1],a))\n",
    "        w=int(valid_w(x,l[2],a,W))\n",
    "        h=int(valid_h(y,l[3],a,H))\n",
    "        a=gray[y:y+h,x:x+w]\n",
    "        if a.size>0:\n",
    "            fr_count=fr_count+1\n",
    "            \n",
    "        # Draw a rectangle around the Faces\n",
    "        #for (x, y, w, h) in faces:\n",
    "            #cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the resulting Frame\n",
    "        #cv2.imshow('Video', frame)\n",
    "    \n",
    "    f=f+count\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "print(\"Total time required:\")\n",
    "print(t)\n",
    "print(\"total frames:\")\n",
    "print(f)\n",
    "print(\"Total faces detected successfully:\")\n",
    "print(fr_count)\n",
    "print(\"Success rate:\")\n",
    "print(fr_count/f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Haar Cascades:\n",
      "Total frames:\n",
      "9537\n",
      "Total time required:\n",
      "142.09819078445435\n",
      "Time per frame:\n",
      "0.014899673983900005\n"
     ]
    }
   ],
   "source": [
    "print(\"For Haar Cascades:\")\n",
    "print(\"Total frames:\")\n",
    "print(f)\n",
    "print(\"Total time required:\")\n",
    "print(t)\n",
    "print(\"Time per frame:\")\n",
    "print(t/f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9537"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=['777.mp4',\n",
    " '033.mp4',\n",
    " '314.mp4',\n",
    " '737.mp4',\n",
    " '255.mp4',\n",
    " '539.mp4',\n",
    " '545.mp4',\n",
    " '321.mp4',\n",
    " '791.mp4',\n",
    " '422.mp4',\n",
    " '349.mp4',\n",
    " '260.mp4',\n",
    " '574.mp4',\n",
    " '363.mp4',\n",
    " '608.mp4',\n",
    " '874.mp4',\n",
    " '632.mp4',\n",
    " '754.mp4',\n",
    " '793.mp4',\n",
    " '900.mp4']\n",
    "import cv2\n",
    "total=0\n",
    "for videos in v:\n",
    "    p=os.path.join(path,videos)\n",
    "    cap = cv2.VideoCapture(p)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    total=total+length\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of extract_faces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
